# DANDI Compute (Code)

Contains essential code for performing computations on DANDI assets.


## Dandiset `001697` Layout

The outer level of the Dandiset is organized according to a [BIDS-Study](https://bids-specification.readthedocs.io/en/stable/common-principles.html#study-dataset) layout.

The `sourcedata` contains only a single unique file: the AIND testing sample. This is used for testing and demonstration purposes.

The primary `derivatives` directory follows the [Type 1 BIDS-Derivatives](https://bids-specification.readthedocs.io/en/stable/common-principles.html#storage-of-derived-datasets) structure with nested derivative directories.

It contains one subdirectory per compute pipeline, starting with `pipeline-aind+ephys`.

Each `pipeline-[identifier]/derivatives` directory contains a subdirectory for each unique asset processed.

Each asset directory contains subdirectories for each unique result generated by different configuration options.

The contents of each `asset-[content ID]/results-[identifier]` directory follow [Type 2 BIDS-Derivatives](https://bids-specification.readthedocs.io/en/stable/common-principles.html#storage-of-derived-datasets) standalone dataset style.

**Note**: To avoid nomenclature confusion with the official asset or other IDs, he "content ID" is defined to be the unique identifier of the object on the S3 bucket. This equivalent to the `.blob` if asset is a standard file, and the `.zarr` if asset is a Zarr store.

The `dataset_description.json` of each result directory contains the provenance information regarding the pipeline used to generate that output.



## Dispatch: AIND on Engaging

To run manually with confirmation to trigger (for debugging):

```bash
dandicompute aind prepare --id [full content ID]
```

To run automatically:

```bash
dandicompute aind prepare --id [full content ID] --submit
```

To test automatically on the example asset:

```bash
dandicompute aind prepare --id 048d1ee9-83b7-491f-8f02-1ca615b1d455 --submit
```
