#!/bin/bash
#SBATCH --job-name=AIND-Ephys-Pipeline
#SBATCH --output={{ log_directory }}/job-%j_slurm.log
#SBATCH --mem=4GB
#SBATCH --partition=mit_normal
#SBATCH --time=12:00:00

BLOB_ID={{ blob_id }}
RUN_ID={{ run_id }}
CONFIG_PATH={{ config_path }}
PIPELINE_PATH={{ pipeline_path }}

PARTITION={{ partition }}

BASE_DANDI_DIR="/orcd/data/dandi/001"
DANDI_COMPUTE_BASE_DIR="$BASE_DANDI_DIR/dandi-compute"
DANDI_COMPUTE_CODE_DIR="$DANDI_COMPUTE_BASE_DIR/code"
DANDISET_DIR="$DANDI_COMPUTE_BASE_DIR/001697"

WORKDIR="$DANDI_COMPUTE_BASE_DIR/work"
NXF_APPTAINER_CACHEDIR="$WORKDIR/apptainer_cache"

DANDI_ARCHIVE_DIR="/orcd/data/dandi/$PARTITION/s3dandiarchive"
NWB_FILE_PATH="$DANDI_ARCHIVE_DIR/blobs/${BLOB_ID:0:3}/${BLOB_ID:3:3}/$BLOB_ID"
RESULTS_PATH="$DANDISET_DIR/pipeline-aind+ephys/blob-$BLOB_ID/run-$RUN_ID/results"
DATA_PATH="$(dirname "$NWB_FILE_PATH")"

if [ -d "$RESULTS_PATH" ]; then
    echo "Error: Run directory already exists at $RESULTS_PATH"
    echo "Please use a different RUN ID or remove the existing directory."
    exit 1
fi

source /etc/profile.d/modules.sh
module load miniforge
module load apptainer

conda activate /orcd/data/dandi/001/environments/name-nextflow_environment

DATA_PATH="$DATA_PATH" RESULTS_PATH="$RESULTS_PATH" NXF_APPTAINER_CACHEDIR="$NXF_APPTAINER_CACHEDIR" nextflow \
    -C {{ config_file_path }} \
    -log "{{ log_directory }}/nextflow.log" \
    run {{ pipeline_file_path }} \
    -work-dir "$WORKDIR" \
    --job_dispatch_args "--input nwb --nwb-files $NWB_FILE_PATH" \
    --nwb_ecephys_args "--backend hdf5"

# TODO: mv various items into temp Dandiset results folder and upload
